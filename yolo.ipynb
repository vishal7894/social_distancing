{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import imutils\n",
    "import math\n",
    "from itertools import combinations\n",
    "\n",
    "# Euclidean distance calculation fuction with points (x_1,y_1) and (x_2,y_2)\n",
    "def distance(x_1,y_1,x_2,y_2):\n",
    "    dist = ((x_1-x_2)**2 + (y_1-y_2)**2)\n",
    "    return math.sqrt(dist)\n",
    "\n",
    "# Function to identity interested points (used to get aspect ratio and corner points)\n",
    "def mouse_points(event,x,y,flags,params):\n",
    "    if event == cv2.EVENT_LBUTTONDBLCLK:\n",
    "        print(x,y)\n",
    "\n",
    "wht = 320   # width, height , target - all set to 320\n",
    "\n",
    "path = 'pedestrian_walking01.mkv'  # path to video feed\n",
    "# path = 'pedestrian_walking02.mkv'\n",
    "# path = 'pedestrian_walking03.mkv'\n",
    "\n",
    "# initializing webcam/video feed\n",
    "cap = cv2.VideoCapture(path)\n",
    "\n",
    "# pre-trained configuration and weights\n",
    "model_config = 'yolov3_320.cfg'       # yolov3_tiny.cfg also used for faster fps but low accuracy\n",
    "model_weights = 'yolov3_320.weights'  # yolov3-tiny.weights also used for faster fps and low accuracy\n",
    "\n",
    "# model_config = 'yolo-obj.cfg'\n",
    "# model_weights = 'yolo-obj_5000.weights'\n",
    "\n",
    "# Network\n",
    "net = cv2.dnn.readNetFromDarknet(model_config,model_weights)\n",
    "net.setPreferableBackend(cv2.dnn.DNN_BACKEND_OPENCV)\n",
    "net.setPreferableTarget(cv2.dnn.DNN_TARGET_CPU)\n",
    "\n",
    "# Thresholds\n",
    "conf_threshold = 0.3   # confidence threshold\n",
    "nms_threshold = 0.1    # Non-maxima supression threshold\n",
    "dist_threshold = 150   # Safe distance threshold (applied randomly)\n",
    "\n",
    "# for polylines on frame of video feed (yellow lines)\n",
    "pts = np.asarray([[475,15],[823,29],[738,447],[47,269]],np.int32)\n",
    "pts = pts.reshape((-1, 1, 2)) \n",
    "\n",
    "# Transformation matrix\n",
    "pts_1 = np.float32([[475,15],[823,29],[738,447],[47,269]]) # points are taken from the video by using mouse_points function\n",
    "pts_2 = np.float32([[0,0],[700,0],[700,800],[0,800]])      # aspect ratio: calculated using points taken from mouse_points function\n",
    "matrix = cv2.getPerspectiveTransform(pts_1,pts_2)\n",
    "\n",
    "def find_objects(outputs,img,frame,matrix):\n",
    "    ht,wt,ct = frame.shape # target shapes\n",
    "    bbox = []              # Bounding boxes\n",
    "    class_Ids = []         # class_Ids (tags)\n",
    "    confs = []             # confidence in object detection\n",
    "\n",
    "    for output in outputs:\n",
    "        for detection in output:\n",
    "            scores = detection[5:]\n",
    "            class_Id = np.argmax(scores)\n",
    "            confidence = scores[class_Id]\n",
    "            if confidence > conf_threshold and class_Id ==0:\n",
    "                w, h = int(detection[2]*wt), int(detection[3]*ht)                  # multiplying with wt and ht as detection is in percentages\n",
    "                x, y = int((detection[0]*wt) - w/2),int((detection[1]*ht)-h/2)\n",
    "                cx, cy = int(detection[0]*wt), int(detection[1]*ht)                # centre X and ground Y\n",
    "                arr = np.asarray([[cx],\n",
    "                                  [int(cy+h/2)],\n",
    "                                  [1]])\n",
    "                warp_arr = matrix@arr                                              # warp_arr contains the tranformation of the centre X and ground Y points on to                                                                                       # the warped plane.\n",
    "\n",
    "                bbox.append([x,y,w,h,cx,cy,int(warp_arr[0][0]*0.479),int(warp_arr[1][0]*0.453)]) # scaling ratio - ratio of width and height in original feed and \n",
    "                                                                                                 # the warped width and height.\n",
    "                class_Ids.append(class_Id)\n",
    "                confs.append(float(confidence))\n",
    "    \n",
    "    indices = cv2.dnn.NMSBoxes(bbox,confs,conf_threshold,nms_threshold)\n",
    "    \n",
    "    dist_dict = {}        # contains person_ID w.r.t frame and their warp tranform coordinates\n",
    "    for i in indices:\n",
    "        i = i[0]\n",
    "        box = bbox[i]\n",
    "        x,y,w,h,cx,cy,wcx,wcy = box[0],box[1],box[2],box[3],box[4],box[5],box[6],box[7]\n",
    "        cv2.rectangle(frame,(x,y),(x+w,y+h),(255,0,255),2)\n",
    "        cv2.circle(frame,(int(cx),int(cy+(h/2))),2,(0,0,255),-1)\n",
    "        cv2.circle(img,(int(wcx),int(wcy)),3,(0,0,255),-1)\n",
    "        \n",
    "        dist_dict[i] = (box[6],box[7])\n",
    "        \n",
    "    red_zone = []       # All IDs not maintaining social distance threshold w.r.t current frame.\n",
    "    green_zone = []     # All IDs maintaining social distance threshold w.rt. current frame.\n",
    "    for (id_1,p_1),(id_2,p_2) in combinations(dist_dict.items(),2):\n",
    "        dx, dy = p_1[0]-p_2[0], p_1[1]-p_2[1]\n",
    "        distances = math.sqrt(dx*dx + dy*dy)\n",
    "        if distances < dist_threshold:\n",
    "            if id_1 not in red_zone:\n",
    "                red_zone.append(id_1)\n",
    "            if id_2 not in red_zone:\n",
    "                red_zone.append(id_2)\n",
    "        else:\n",
    "            if id_1 not in red_zone:\n",
    "                green_zone.append(id_1)\n",
    "            if id_2 not in red_zone:\n",
    "                green_zone.append(id_2)\n",
    "        \n",
    "    if len(red_zone)>=2:\n",
    "        for i,j in combinations(red_zone,2):\n",
    "            cv2.line(img,dist_dict[i],dist_dict[j],(0,0,255),2)\n",
    "    if len(green_zone)>=2:\n",
    "        for i,j in combinations(green_zone,2):\n",
    "            cv2.line(img,dist_dict[i],dist_dict[j],(0,255,0),1)\n",
    "\n",
    "    \n",
    "while True:\n",
    "    _,frame = cap.read()\n",
    "    \n",
    "    blob = cv2.dnn.blobFromImage(frame,1/255,(wht,wht),[0,0,0],1,crop = False)\n",
    "    net.setInput(blob)\n",
    "    \n",
    "    layer_names = net.getLayerNames()\n",
    "    output_names = [layer_names[i[0]-1] for i in net.getUnconnectedOutLayers()]\n",
    "    \n",
    "    cv2.polylines(frame,[pts],isClosed = True,color = (0,255,255),thickness = 1)\n",
    "    outputs = net.forward(output_names)\n",
    "\n",
    "    img = cv2.warpPerspective(frame,matrix,(700,800))\n",
    "    img[0:800,0:700] = [0,0,0]\n",
    "\n",
    "    find_objects(outputs,img,frame,matrix)\n",
    "\n",
    "    img_ = imutils.resize(img,480)\n",
    "    cv2.imshow('sample',frame)\n",
    "    cv2.imshow('warped',img_)\n",
    "    key = cv2.waitKey(1)\n",
    "    \n",
    "    if key == ord('q'):\n",
    "        break\n",
    "        \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
